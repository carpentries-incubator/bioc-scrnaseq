---
title: Multi-sample analyses
teaching: 30 # Minutes of teaching in the lesson
exercises: 15 # Minutes of exercises in the lesson
---

:::::::::::::::::::::::::::::::::::::: questions 

- How can we integrate data from multiple batches, samples, and studies?
- How can we identify differentially expressed genes between experimental conditions for each cell type?
- How can we identify changes in cell type abundance between experimental conditions?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Correct batch effects and diagnose potential problems such as over-correction.
- Perform differential expression comparisons between conditions based on pseudo-bulk samples.
- Perform differential abundance comparisons between conditions.

::::::::::::::::::::::::::::::::::::::::::::::::


## Setup and data exploration

As before, we will use the the wild-type data from the Tal1 chimera experiment:

- Sample 5: E8.5 injected cells (tomato positive), pool 3
- Sample 6: E8.5 host cells (tomato negative), pool 3
- Sample 7: E8.5 injected cells (tomato positive), pool 4
- Sample 8: E8.5 host cells (tomato negative), pool 4
- Sample 9: E8.5 injected cells (tomato positive), pool 5
- Sample 10: E8.5 host cells (tomato negative), pool 5

Note that this is a paired design in which for each biological replicate (pool 3, 4, and 5), we have both host and injected cells.

We start by loading the data and doing a quick exploratory analysis, essentially applying the normalization and visualization techniques that we have seen in the previous lectures to all samples. Note that this time we're selecting samples 5 to 10, not just 5 by itself.

```{r chunk-opts, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(BiocStyle)
```

```{r setup}
library(MouseGastrulationData)
library(batchelor)
library(edgeR)
library(scater)
library(ggplot2)
library(scran)
library(pheatmap)
library(scuttle)

sce <- WTChimeraData(samples = 5:10, type = "processed")

sce

colData(sce)
```

To speed up computations, after removing doublets, we randomly select 50% cells per sample.

```{r}
drop <- sce$celltype.mapped %in% c("stripped", "Doublet")

sce <- sce[,!drop]

set.seed(29482)

idx <- unlist(tapply(colnames(sce), sce$sample, function(x) {
    perc <- round(0.50 * length(x))
    sample(x, perc)
}))

sce <- sce[,idx]
```

We now normalize the data, run some dimensionality reduction steps, and visualize them in a tSNE plot.

```{r}
sce <- logNormCounts(sce)

dec <- modelGeneVar(sce, block = sce$sample)

chosen.hvgs <- dec$bio > 0

sce <- runPCA(sce, subset_row = chosen.hvgs, ntop = 1000)

sce <- runTSNE(sce, dimred = "PCA")

sce$sample <- as.factor(sce$sample)

plotTSNE(sce, colour_by = "sample")

plotTSNE(sce, colour_by = "celltype.mapped") +
    scale_color_discrete() +
    theme(legend.position = "bottom")
```

There are evident sample effects. Depending on the analysis that you want to perform you may want to remove or retain the sample effect. For instance, if the goal is to identify cell types with a clustering method, one may want to remove the sample effects with "batch effect" correction methods.

For now, let's assume that we want to remove this effect.

:::: challenge

It seems like samples 5 and 6 are separated off from the others in gene expression space. Given the group of cells in each sample, why might this make sense versus some other pair of samples? What is the factor presumably leading to this difference?

::: solution

Samples 5 and 6 were from the same "pool" of cells. Looking at the documentation for the dataset under `?WTChimeraData` we see that the pool variable is defined as: "Integer, embryo pool from which cell derived; samples with same value are matched." So samples 5 and 6 have an experimental factor in common which causes a shared, systematic difference in gene expression profiles compared to the other samples. That's why you can see many of isolated blue/orange clusters on the first TSNE plot. If you were developing single-cell library preparation protocols you might want to preserve this effect to understand how variation in pools leads to variation in expression, but for now, given that we're investigating other effects, we'll want to remove this as undesired technical variation.

:::

::::

## Correcting batch effects

We "correct" the effect of samples with the `correctExperiment` function
in the `batchelor` package and using the `sample` column as batch.


```{r}
set.seed(10102)

merged <- correctExperiments(
    sce, 
    batch = sce$sample, 
    subset.row = chosen.hvgs,
    PARAM = FastMnnParam(
        merge.order = list(
            list(1,3,5), # WT (3 replicates)
            list(2,4,6)  # td-Tomato (3 replicates)
        )
    )
)

merged <- runTSNE(merged, dimred = "corrected")

plotTSNE(merged, colour_by = "batch")

```

Once we removed the sample batch effect, we can proceed with the Differential 
Expression Analysis.

:::: challenge

True or False: after batch correction, no batch-level information is present in the corrected data.

::: solution

False. Batch-level data can be retained through confounding with experimental factors or poor ability to distinguish experimental effects from batch effects. Remember, the changes needed to correct the data are empirically estimated, so they can carry along error. 

While batch effect correction algorithms usually do a pretty good job, it's smart to do a sanity check for batch effects at the end of your analysis. You always want to make sure that that effect you're resting your paper submission on isn't driven by batch effects.

:::

::::


## Differential Expression

In order to perform a differential expression analysis, we need to identify 
groups of cells across samples/conditions (depending on the experimental 
design and the final aim of the experiment). 

As previously seen, we have two ways of grouping cells, cell clustering and cell
labeling. In our case we will focus on this second aspect to group cells
according to the already annotated cell types to proceed with the computation of
the pseudo-bulk samples.

### Pseudo-bulk samples

To compute differences between groups of cells, a possible way is to compute
pseudo-bulk samples, where we mediate the gene signal of all the cells for each
specific cell type. In this manner, we are then able to detect differences
between the same cell type across two different conditions.

To compute pseudo-bulk samples, we use the `aggregateAcrossCells` function in the 
`scuttle` package, which takes as input not only a SingleCellExperiment, 
but also the id to use for the identification of the group of cells.
In our case, we use as id not just the cell type, but also the sample, because
we want be able to discern between replicates and conditions during further steps.

```{r}
# Using 'label' and 'sample' as our two factors; each column of the output
# corresponds to one unique combination of these two factors.

summed <- aggregateAcrossCells(
    merged, 
    id = colData(merged)[,c("celltype.mapped", "sample")]
)

summed

```

### Differential Expression Analysis

The main advantage of using pseudo-bulk samples is the possibility to use
well-tested methods for differential analysis like `edgeR` and `DESeq2`, we will
focus on the former for this analysis. `edgeR` and `DESeq2` both use negative binomial models under the hood, but differ in their normalization strategies and other implementation details.

First, let's start with a specific cell type, for instance the "Mesenchymal stem
cells", and look into differences between this cell type across conditions. We put the counts table into a `DGEList` container called `y`, along with the corresponding metadata.

```{r}
current <- summed[, summed$celltype.mapped == "Mesenchyme"]

y <- DGEList(counts(current), samples = colData(current))

y
```

A typical step is to discard low quality samples due to low sequenced library
size. We discard these samples because they can affect further steps like
normalization and/or DEGs analysis.

We can see that in our case we don't have low quality samples and we don't need 
to filter out any of them.

```{r}
discarded <- current$ncells < 10

y <- y[,!discarded]

summary(discarded)
```

The same idea is typically applied to the genes, indeed we need to discard low 
expressed genes to improve accuracy for the DEGs modeling.

```{r}
keep <- filterByExpr(y, group = current$tomato)

y <- y[keep,]

summary(keep)
```

We can now proceed to normalize the data. There are several approaches for
normalizing bulk, and hence pseudo-bulk data. Here, we use the Trimmed Mean of
M-values method, implemented in the `edgeR` package within the `calcNormFactors`
function. Keep in mind that because we are going to normalize the pseudo-bulk
counts, we don't need to normalize the data in "single cell form".

```{r}
y <- calcNormFactors(y)

y$samples
```

To investigate the effect of our normalization, we use a Mean-Difference (MD)
plot for each sample in order to detect possible normalization problems due to
insufficient cells/reads/UMIs composing a particular pseudo-bulk profile.

In our case, we verify that all these plots are centered in 0 (on y-axis) and
present a trumpet shape, as expected.


```{r}
par(mfrow = c(2,3))

for (i in seq_len(ncol(y))) {
    plotMD(y, column = i)
}

par(mfrow = c(1,1))
```

Furthermore, we want to check if the samples cluster together based
on their known factors (like the tomato injection in this case).

In this case, we'll use the multidimensional scaling (MDS) plot. Multidimensional scaling (which also goes by principal *coordinate* analysis (PCoA)) is a dimensionality reduction technique that's conceptually similar to principal *component* analysis (PCA).
    
```{r}
limma::plotMDS(cpm(y, log = TRUE), 
               col = ifelse(y$samples$tomato, "red", "blue"))
```

We then construct a design matrix by including both the pool and the tomato as factors.
This design indicates which samples belong to which pool and condition, so we can
use it in the next step of the analysis.

```{r}
design <- model.matrix(~factor(pool) + factor(tomato),
                       data = y$samples)
design
```

Now we can estimate the Negative Binomial (NB) overdispersion parameter, to model
the mean-variance trend.

```{r}
y <- estimateDisp(y, design)

summary(y$trended.dispersion)
```

The BCV plot allows us to investigate the relation between the Biological Coefficient
of Variation and the Average log CPM for each gene.
Additionally, the Common and Trend BCV are shown in `red` and `blue`.

```{r}
plotBCV(y)
```

We then fit a Quasi-Likelihood (QL) negative binomial generalized linear model for each gene. 
The `robust = TRUE` parameter avoids distortions from highly variable clusters.
The QL method includes an additional dispersion parameter, useful to handle the uncertainty and variability of the per-gene variance, which is not well estimated by the NB dispersions, so the two dispersion types complement each other in the final analysis.

```{r}
fit <- glmQLFit(y, design, robust = TRUE)

summary(fit$var.prior)

summary(fit$df.prior)
```

QL dispersion estimates for each gene as a function of abundance. Raw estimates (black) are shrunk towards the trend (blue) to yield squeezed estimates (red).

```{r}
plotQLDisp(fit)
```

We then use an empirical Bayes quasi-likelihood F-test to test for differential expression (due to tomato injection) per each gene at a False Discovery Rate (FDR) of 5%.
The low amount of DGEs highlights that the tomato injection effect has a low 
influence on the mesenchyme cells.

```{r}
res <- glmQLFTest(fit, coef = ncol(design))

summary(decideTests(res))

topTags(res)
```

All the previous steps can be easily performed with the following function 
for each cell type, thanks to the `pseudoBulkDGE` function in the `scran` package.

```{r}
summed.filt <- summed[,summed$ncells >= 10]

de.results <- pseudoBulkDGE(
    summed.filt, 
    label = summed.filt$celltype.mapped,
    design = ~factor(pool) + tomato,
    coef = "tomatoTRUE",
    condition = summed.filt$tomato 
)
```

The returned object is a list of `DataFrame`s each with the results for a cell type.
Each of these contains also the intermediate results in `edgeR` format to perform any intermediate plot or diagnostic.

```{r}
cur.results <- de.results[["Allantois"]]

cur.results[order(cur.results$PValue),]
```

:::: challenge

Clearly some of the results have low p-values. What about the effect sizes? What does `logFC` stand for?

::: solution

"logFC" stands for log fold-change. `edgeR` uses a log2 convention. Rather than reporting e.g. a 5-fold increase, it's better to report a logFC of log2(5) = 2.32. Additive log scales are easier to work with than multiplicative identity scales, once you get used to it.

`ENSMUSG00000037664` seems to have an estimated logFC of about -8. That's a big difference if it's real.

:::

::::


## Differential Abundance

With DA we look for differences in cluster *abundance* across conditions (the
tomato injection in our case), rather than differences in gene expression.

Our first steps are quantifying the number of cells per each cell type and
fitting a model to catch differences between the injected cells and the
background.

The process is very similar differential expression modeling, but this time we
start our analysis on the computed abundances and without normalizing the data
with TMM.

```{r}
abundances <- table(merged$celltype.mapped, merged$sample) 

abundances <- unclass(abundances) 

extra.info <- colData(merged)[match(colnames(abundances), merged$sample),]

y.ab <- DGEList(abundances, samples = extra.info)

design <- model.matrix(~factor(pool) + factor(tomato), y.ab$samples)

y.ab <- estimateDisp(y.ab, design, trend = "none")

fit.ab <- glmQLFit(y.ab, design, robust = TRUE, abundance.trend = FALSE)
```

### Background on compositional effect

As mentioned before, in DA we don't normalize our data with `calcNormFactors`
function, because this approach considers that most of the input features do not
vary between conditions. This cannot be applied to DA analysis because we have a
small number of cell populations that all can change due to the treatment. This
means that here we will normalize only for library depth, which in pseudo-bulk
data means by the total number of cells in each sample (cell type).

On the other hand, this can lead our data to be susceptible to compositional
effect. "Compositional" refers to the fact that the cluster abundances in a
sample are not independent of one another because each cell type is effectively
competing for space in the sample. They behave like proportions in that they
must sum to 1. If cell type A abundance increases in a new condition, that means
we'll observe less of everything else, even if everything else is unaffected by
the new condition.

Compositionality means that our conclusions can be biased by the amount of cells
present in each cell type. And this amount of cells can be totally unbalanced
between cell types. This is particularly problematic for cell types that start
at or end up near 0 or 100 percent.

For example, a specific cell type can be 40% of the total amount of cells
present in the experiment, while another just the 3%. The differences in terms
of abundance of these cell types are detected between the different conditions,
but our final interpretation could be biased if we don't consider this aspect.

We now look at different approaches for handling the compositional effect.

### Assuming most labels do not change

We can use a similar approach used during the DEGs analysis, assuming that most
labels are not changing, in particular if we think about the low number of DEGs 
resulted from the previous analysis.

To do so, we first normalize the data with `calcNormFactors` and then we fit and 
estimate a QL-model for our abundance data.

```{r}
y.ab2 <- calcNormFactors(y.ab)

y.ab2$samples$norm.factors
```

We then use edgeR in a manner similar to what we ran before: 

```{r}
y.ab2 <- estimateDisp(y.ab2, design, trend = "none")

fit.ab2 <- glmQLFit(y.ab2, design, robust = TRUE, abundance.trend = FALSE)

res2 <- glmQLFTest(fit.ab2, coef = ncol(design))

summary(decideTests(res2))

topTags(res2, n = 10)
```

###  Testing against a log-fold change threshold

A second approach assumes that the composition bias introduces a spurious
log2-fold change of no more than a \tau quantity for a non-DA label.

In other words, we interpret this as the maximum log-fold change in the total
number of cells given by DA in other labels. On the other hand, when choosing
\tau, we should not consider fold-differences in the totals due to differences
in capture efficiency or the size of the original cell population are not
attributable to composition bias. We then mitigate the effect of composition
biases by testing each label for changes in abundance beyond \tau.

```{r}
res.lfc <- glmTreat(fit.ab, coef = ncol(design), lfc = 1)

summary(decideTests(res.lfc))

topTags(res.lfc)
```

Addionally, the choice of \tau can be guided by other external experimental data, like a previous or a pilot experiment.

## Session Info

```{r, tidy=TRUE}
sessionInfo()
```

## Exercises

:::::::::::::::::::::::::::::::::: challenge

#### Exercise 1: Replicates

Test differential expressed genes with just 2 replicates per condition and look into the changes in the results and possible emerging issues.


:::::::::::::: hint

Remember, you can subset SingleCellExperiments with logical indices, just like a matrix. You can also access their column data with the `$` accessor, like a data frame. 

:::::::::::::::::::::::

:::::::::::::: solution


```{r}

summed.filt.subset = summed.filt[,summed.filt$pool != 3]

de.results <- pseudoBulkDGE(
    summed.filt.subset, 
    label = summed.filt.subset$celltype.mapped,
    design = ~factor(pool) + tomato,
    coef = "tomatoTRUE",
    condition = summed.filt.subset$tomato 
)
```

:::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::::::::

:::::::::::::::::::::::::::::::::: challenge

#### Exercise 2: Heatmaps

Use the `pheatmap` package to create a heatmap of the abundances table. Does it comport with the model results?

:::::::::::::: hint

You can just hand `pheatmap()` a matrix as its only argument. It has a million options, but the defaults are usually pretty good.

:::::::::::::::::::::::

:::::::::::::: solution

```{r}
pheatmap(y.ab$counts)
```

The top DA result was a decrease in ExE ectoderm in the tomato condition, which you can sort of see, especially if you `log1p()` the counts or discard rows that show much higher values. ExE ectoderm counts were much higher in samples 8 and 10 compared to 5, 7, and 9. 

:::::::::::::::::::::::

:::::::::::::::::::::::::::::::::::::::::::::

:::: challenge

#### Extension challenge 1: Group effects

Having multiple independent samples in each experimental group is always helpful, but it particularly important when it comes to batch effect correction. Why?

::: solution

It's important to have multiple samples within each experimental group because it helps the batch effect correction algorithm distinguish differences due to batch effects (uninteresting) from differences due to group/treatment/biology (interesting). 

Imagine you had one sample that received a drug treatment and one that did not, each with 10,000 cells. They differ substantially in expression of gene X. Is that an important scientific finding? You can't tell for sure, because the effect of drug is indistinguishable from a sample-wise batch effect. But if the difference in gene X holds up when you have five treated samples and five untreated samples, now you can be a bit more confident. Many batch effect correction methods will take information on experimental factors as additional arguments, which they can use to help remove batch effects while retaining experimental differences.

:::

::::

:::::::::::::: checklist
## Further Reading

* OSCA book, Multi-sample analysis, [Chapters 1, 4, and 6](https://bioconductor.org/books/release/OSCA.multisample)

::::::::::::::

::::::::::::::::::::::::::::::::::::: keypoints 
-   Batch effects are systematic technical differences in the observed expression
    in cells measured in different experimental batches.
-   Computational removal of batch-to-batch variation with the `correctExperiment`
    function from the `r Biocpkg("batchelor")` package allows us to combine data
    across multiple batches for a consolidated downstream analysis.
-   Differential expression (DE) analysis of replicated multi-condition scRNA-seq experiments
    is typically based on pseudo-bulk expression profiles, generated by summing
    counts for all cells with the same combination of label and sample.
-   The `aggregateAcrossCells` function from the `r Biocpkg("scater")` package
    facilitates the creation of pseudo-bulk samples.   
-   The `pseudoBulkDGE` function from the `r Biocpkg("scran")` package can be used
    to detect significant changes in expression between conditions for pseudo-bulk samples
    consisting of cells of the same type.
-   Differential abundance (DA) analysis aims at identifying significant changes in
    cell type abundance across conditions.
-   DA analysis uses bulk DE methods such as `r Biocpkg("edgeR")` and `r Biocpkg("DESeq2")`,
    which provide suitable statistical models for count data in the presence of
    limited replication - except that the counts are not of reads per gene, but
    of cells per label.
::::::::::::::::::::::::::::::::::::::::::::::::
